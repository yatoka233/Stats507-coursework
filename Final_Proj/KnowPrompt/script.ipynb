{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1332),\n",
       " (1, 1621),\n",
       " (2, 2761),\n",
       " (3, 118),\n",
       " (4, 1235),\n",
       " (5, 161),\n",
       " (6, 3),\n",
       " (7, 157),\n",
       " (8, 47)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "re_count = {}\n",
    "\n",
    "# re2id\n",
    "re2id = json.load(open(\"/nfs/turbo/umms-drjieliu/proj/prompt_re/KnowPrompt/dataset/biored_mapped/rel2id.json\", \"r\", encoding='utf-8'))\n",
    "\n",
    "with open(\"/nfs/turbo/umms-drjieliu/proj/prompt_re/KnowPrompt/dataset/biored_mapped/train.txt\", \"r\", encoding='utf-8') as reader:\n",
    "    all_lines = reader.readlines()\n",
    "    for line in all_lines:\n",
    "        ins = eval(line)\n",
    "        relation = ins[\"relation\"]\n",
    "        if re2id[relation] not in re_count:\n",
    "            re_count[re2id[relation]] = 1\n",
    "        else:\n",
    "            re_count[re2id[relation]] += 1\n",
    "\n",
    "# sort by key\n",
    "re_count = sorted(re_count.items(), key=lambda x: x[0])\n",
    "re_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RobertaForPrompt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"RobertaForPrompt\".rsplit(\".\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dcd13b8b734562a534fce1e157ef8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23fa9ffe9194890b6054d483500f9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=226150.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b519e9706b84c6f86db5e7beeed0e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = [\"[object_start]\", \"[object_end]\", \"[subject_start]\", \"[subject_end]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'cls_token': \"[CLS]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT2TokenizerFast'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'biogpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# \"roberta-large\" from pretrained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/biogpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/knowprompt/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:389\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_auto\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 389\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    390\u001b[0m         pretrained_model_name_or_path, return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    394\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/anaconda3/envs/knowprompt/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:448\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m--> 448\u001b[0m     config_class \u001b[39m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[39m\"\u001b[39;49m\u001b[39mmodel_type\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m config_class\u001b[39m.\u001b[39mfrom_dict(config_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    450\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[39m# Fallback: use pattern matching on the string.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'biogpt'"
     ]
    }
   ],
   "source": [
    "# \"roberta-large\" from pretrained model\n",
    "model = transformers.AutoModel.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special tokens in tokenizer\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask>'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()['<mask>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what token is input_ids 103\n",
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'additional_special_tokens': entity_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[T1]', '[T2]', '[T3]', '[T4]', '[T5]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"[T{i}]\" for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lines /home/dengfy/KnowPrompt-master/dataset/semeval/test.txt\n",
    "with open('/home/dengfy/KnowPrompt-master/dataset/semeval/test.txt', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': ['the',\n",
       "  'most',\n",
       "  'common',\n",
       "  'audits',\n",
       "  'were',\n",
       "  'about',\n",
       "  'waste',\n",
       "  'and',\n",
       "  'recycling',\n",
       "  '.'],\n",
       " 'h': {'name': 'audits', 'pos': [3, 4]},\n",
       " 't': {'name': 'waste', 'pos': [6, 7]},\n",
       " 'relation': 'Message-Topic(e1,e2)'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = eval(lines[0])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audits'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBJECT = \" \".join(example[\"token\"][3: 4])\n",
    "SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 26723, 2, 2, 5247, 2629, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" \"+SUBJECT,SUBJECT, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> audits</s></s>audits</s>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0, 26723, 2, 2, 5247, 2629, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"dataset/semeval/rel2id.json\", \"r\") as file:\n",
    "    t = json.load(file)\n",
    "    label_list = list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "def split_label_words(tokenizer, label_list):\n",
    "    label_word_list = []\n",
    "    for label in label_list:\n",
    "        if label == 'no_relation' or label == \"NA\":\n",
    "            label_word_id = tokenizer.encode('no relation', add_special_tokens=False)\n",
    "            label_word_list.append(torch.tensor(label_word_id))\n",
    "        else:\n",
    "            tmps = label\n",
    "            label = label.lower()\n",
    "            label = label.split(\"(\")[0]\n",
    "            label = label.replace(\":\",\" \").replace(\"_\",\" \").replace(\"per\",\"person\").replace(\"org\",\"organization\")\n",
    "            label_word_id = tokenizer(label, add_special_tokens=False)['input_ids']\n",
    "            print(label, label_word_id)\n",
    "            label_word_list.append(torch.tensor(label_word_id))\n",
    "    padded_label_word_list = pad_sequence([x for x in label_word_list], batch_first=True, padding_value=0)\n",
    "    return padded_label_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component-whole [46362, 12, 11613, 4104]\n",
      "other [7443]\n",
      "instrument-agency [179, 41392, 12, 26904]\n",
      "member-collection [8648, 12, 44443]\n",
      "cause-effect [27037, 12, 26715]\n",
      "entity-destination [46317, 12, 31549, 8111]\n",
      "content-container [10166, 12, 46367]\n",
      "message-topic [44773, 12, 45260]\n",
      "product-producer [20565, 12, 26790, 7742]\n",
      "member-collection [8648, 12, 44443]\n",
      "entity-origin [46317, 12, 43211]\n",
      "cause-effect [27037, 12, 26715]\n",
      "component-whole [46362, 12, 11613, 4104]\n",
      "message-topic [44773, 12, 45260]\n",
      "product-producer [20565, 12, 26790, 7742]\n",
      "entity-origin [46317, 12, 43211]\n",
      "content-container [10166, 12, 46367]\n",
      "instrument-agency [179, 41392, 12, 26904]\n",
      "entity-destination [46317, 12, 31549, 8111]\n"
     ]
    }
   ],
   "source": [
    "t = split_label_words(tokenizer, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46362,    12, 11613,  4104],\n",
       "        [ 7443,     0,     0,     0],\n",
       "        [  179, 41392,    12, 26904],\n",
       "        [ 8648,    12, 44443,     0],\n",
       "        [27037,    12, 26715,     0],\n",
       "        [46317,    12, 31549,  8111],\n",
       "        [10166,    12, 46367,     0],\n",
       "        [44773,    12, 45260,     0],\n",
       "        [20565,    12, 26790,  7742],\n",
       "        [ 8648,    12, 44443,     0],\n",
       "        [46317,    12, 43211,     0],\n",
       "        [27037,    12, 26715,     0],\n",
       "        [46362,    12, 11613,  4104],\n",
       "        [44773,    12, 45260,     0],\n",
       "        [20565,    12, 26790,  7742],\n",
       "        [46317,    12, 43211,     0],\n",
       "        [10166,    12, 46367,     0],\n",
       "        [  179, 41392,    12, 26904],\n",
       "        [46317,    12, 31549,  8111]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [10975, 4684, 134, 742], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"[class1]\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'['"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(10975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = [f\"[class{i}]\" for i in range(1, 19+1)]\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': class_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50265,\n",
       " 50266,\n",
       " 50267,\n",
       " 50268,\n",
       " 50269,\n",
       " 50270,\n",
       " 50271,\n",
       " 50272,\n",
       " 50273,\n",
       " 50274,\n",
       " 50275,\n",
       " 50276,\n",
       " 50277,\n",
       " 50278,\n",
       " 50279,\n",
       " 50280,\n",
       " 50281,\n",
       " 50282,\n",
       " 50283]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a[0] for a in tokenizer([f\"[class{i}]\" for i in range(1, 19+1)], add_special_tokens=False)['input_ids']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
